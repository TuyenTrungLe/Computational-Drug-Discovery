{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADMET Safety Model - Multi-Property Drug Safety Prediction\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a comprehensive **ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity)** safety filtering system using Random Forest models. This is **Stage 2** of the drug discovery pipeline.\n",
    "\n",
    "### Features:\n",
    "- Multi-task ADMET prediction (Toxicity, Clinical Toxicity, BBB Permeability, Solubility)\n",
    "- RDKit molecular descriptor calculation\n",
    "- Random Forest classification and regression\n",
    "- Model persistence and loading\n",
    "- Comprehensive evaluation metrics\n",
    "- SMILES-based compound filtering\n",
    "\n",
    "### Datasets Used:\n",
    "- **Tox21**: Toxicity prediction (12 targets)\n",
    "- **ClinTox**: Clinical trial toxicity\n",
    "- **BBBP**: Blood-Brain Barrier Permeability\n",
    "- **ESOL (Delaney)**: Aqueous Solubility\n",
    "\n",
    "**Author:** Bio-ScreenNet Team  \n",
    "**Date:** 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gzip\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    mean_squared_error, r2_score, mean_absolute_error\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# RDKit imports\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors, AllChem\n",
    "    from rdkit import RDLogger\n",
    "    RDLogger.DisableLog('rdApp.*')  # Disable RDKit warnings\n",
    "    print(\"✓ RDKit imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"Warning: RDKit not installed. Please install: conda install -c conda-forge rdkit\")\n",
    "    sys.exit(1)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Molecular Descriptor Calculator\n",
    "\n",
    "This class calculates molecular descriptors from SMILES strings using RDKit.\n",
    "\n",
    "### Features Calculated:\n",
    "- **Lipinski descriptors**: Molecular Weight, LogP, H-bond Donors/Acceptors, TPSA\n",
    "- **Structural features**: Rotatable bonds, Aromatic/Aliphatic rings\n",
    "- **Morgan Fingerprints**: 512-bit circular fingerprints (radius=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularDescriptorCalculator:\n",
    "    \"\"\"Calculate molecular descriptors from SMILES strings using RDKit.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_descriptors(smiles: str) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Calculate molecular descriptors for a given SMILES string.\n",
    "\n",
    "        Args:\n",
    "            smiles: SMILES representation of molecule\n",
    "\n",
    "        Returns:\n",
    "            Array of molecular descriptors or None if calculation fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return None\n",
    "\n",
    "            # Calculate Lipinski descriptors\n",
    "            mw = Descriptors.MolWt(mol)\n",
    "            logp = Descriptors.MolLogP(mol)\n",
    "            hbd = Descriptors.NumHDonors(mol)\n",
    "            hba = Descriptors.NumHAcceptors(mol)\n",
    "            tpsa = Descriptors.TPSA(mol)\n",
    "\n",
    "            # Additional descriptors\n",
    "            rot_bonds = Descriptors.NumRotatableBonds(mol)\n",
    "            aromatic_rings = Descriptors.NumAromaticRings(mol)\n",
    "            aliphatic_rings = Descriptors.NumAliphaticRings(mol)\n",
    "\n",
    "            # Fingerprint-based descriptors (Morgan fingerprint)\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=512)\n",
    "            fp_array = np.array(fp)\n",
    "\n",
    "            # Combine all descriptors\n",
    "            basic_descriptors = np.array([\n",
    "                mw, logp, hbd, hba, tpsa, rot_bonds,\n",
    "                aromatic_rings, aliphatic_rings\n",
    "            ])\n",
    "\n",
    "            descriptors = np.concatenate([basic_descriptors, fp_array])\n",
    "            return descriptors\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating descriptors for {smiles}: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def batch_calculate_descriptors(smiles_list: List[str], show_progress: bool = True) -> Tuple[np.ndarray, List[int]]:\n",
    "        \"\"\"\n",
    "        Calculate descriptors for a list of SMILES.\n",
    "\n",
    "        Args:\n",
    "            smiles_list: List of SMILES strings\n",
    "            show_progress: Whether to show progress bar\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (descriptor_matrix, valid_indices)\n",
    "        \"\"\"\n",
    "        descriptors = []\n",
    "        valid_indices = []\n",
    "\n",
    "        iterator = tqdm(enumerate(smiles_list), total=len(smiles_list), desc=\"Calculating descriptors\") if show_progress else enumerate(smiles_list)\n",
    "\n",
    "        for idx, smiles in iterator:\n",
    "            desc = MolecularDescriptorCalculator.calculate_descriptors(smiles)\n",
    "            if desc is not None:\n",
    "                descriptors.append(desc)\n",
    "                valid_indices.append(idx)\n",
    "\n",
    "        return np.array(descriptors), valid_indices\n",
    "\n",
    "print(\"✓ MolecularDescriptorCalculator class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ADMET Safety Model Class\n",
    "\n",
    "This is the main class that handles all ADMET prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADMETSafetyModel:\n",
    "    \"\"\"\n",
    "    Comprehensive ADMET Safety Prediction Model.\n",
    "\n",
    "    This class handles multiple ADMET properties including:\n",
    "    - Toxicity (Tox21)\n",
    "    - Clinical Toxicity (ClinTox)\n",
    "    - Blood-Brain Barrier Permeability (BBBP)\n",
    "    - Aqueous Solubility (ESOL)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str = None, model_dir: str = None):\n",
    "        \"\"\"\n",
    "        Initialize ADMET Safety Model.\n",
    "\n",
    "        Args:\n",
    "            data_dir: Directory containing ADMET datasets\n",
    "            model_dir: Directory to save/load trained models\n",
    "        \"\"\"\n",
    "        if data_dir is None:\n",
    "            data_dir = os.path.join(os.path.expanduser(\"~\"), \".deepchem\", \"datasets\")\n",
    "        if model_dir is None:\n",
    "            # Use relative path from notebook location\n",
    "            model_dir = os.path.join(\"..\", \"models\", \"admet_models\")\n",
    "\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.model_dir = Path(model_dir)\n",
    "        self.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.feature_names = None\n",
    "        self.results = {}\n",
    "\n",
    "        print(f\"ADMET Model initialized\")\n",
    "        print(f\"Data directory: {self.data_dir}\")\n",
    "        print(f\"Model directory: {self.model_dir}\")\n",
    "\n",
    "    def load_dataset(self, dataset_name: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load ADMET dataset from file.\n",
    "\n",
    "        Args:\n",
    "            dataset_name: Name of dataset (tox21, clintox, bbbp, sider, delaney)\n",
    "\n",
    "        Returns:\n",
    "            DataFrame containing the dataset or None if loading fails\n",
    "        \"\"\"\n",
    "        dataset_files = {\n",
    "            'tox21': 'tox21.csv.gz',\n",
    "            'clintox': 'clintox.csv.gz',\n",
    "            'bbbp': 'BBBP.csv',\n",
    "            'sider': 'sider.csv.gz',\n",
    "            'delaney': 'delaney-processed.csv'\n",
    "        }\n",
    "\n",
    "        if dataset_name not in dataset_files:\n",
    "            print(f\"Unknown dataset: {dataset_name}\")\n",
    "            return None\n",
    "\n",
    "        file_path = self.data_dir / dataset_files[dataset_name]\n",
    "\n",
    "        if not file_path.exists():\n",
    "            print(f\"Dataset file not found: {file_path}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            print(f\"\\nLoading {dataset_name} dataset from {file_path}...\")\n",
    "\n",
    "            if file_path.suffix == '.gz':\n",
    "                df = pd.read_csv(file_path, compression='gzip')\n",
    "            else:\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "            print(f\"Loaded {len(df)} samples from {dataset_name}\")\n",
    "            print(f\"Columns: {list(df.columns)}\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {dataset_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def prepare_data(self, df: pd.DataFrame, smiles_col: str, target_cols: List[str]) -> Tuple:\n",
    "        \"\"\"\n",
    "        Prepare data for training by calculating molecular descriptors.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame containing SMILES and target columns\n",
    "            smiles_col: Name of SMILES column\n",
    "            target_cols: Names of target columns\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (X, y, valid_df)\n",
    "        \"\"\"\n",
    "        print(f\"\\nPreparing data...\")\n",
    "        print(f\"SMILES column: {smiles_col}\")\n",
    "        print(f\"Target columns: {target_cols}\")\n",
    "\n",
    "        # Calculate descriptors\n",
    "        X, valid_indices = MolecularDescriptorCalculator.batch_calculate_descriptors(\n",
    "            df[smiles_col].tolist(), show_progress=True\n",
    "        )\n",
    "\n",
    "        # Filter valid samples\n",
    "        valid_df = df.iloc[valid_indices].reset_index(drop=True)\n",
    "        y = valid_df[target_cols].values\n",
    "\n",
    "        # Remove samples with missing targets\n",
    "        valid_mask = ~np.isnan(y).any(axis=1)\n",
    "        X = X[valid_mask]\n",
    "        y = y[valid_mask]\n",
    "        valid_df = valid_df[valid_mask].reset_index(drop=True)\n",
    "\n",
    "        print(f\"Final dataset: {len(X)} samples with {X.shape[1]} features\")\n",
    "        print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "        return X, y, valid_df\n",
    "\n",
    "    def save_model(self, model_name: str, model, scaler):\n",
    "        \"\"\"Save trained model and scaler to disk.\"\"\"\n",
    "        model_path = self.model_dir / f\"{model_name}_model.pkl\"\n",
    "        scaler_path = self.model_dir / f\"{model_name}_scaler.pkl\"\n",
    "\n",
    "        joblib.dump(model, model_path)\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    def load_model(self, model_name: str) -> bool:\n",
    "        \"\"\"Load trained model and scaler from disk.\"\"\"\n",
    "        model_path = self.model_dir / f\"{model_name}_model.pkl\"\n",
    "        scaler_path = self.model_dir / f\"{model_name}_scaler.pkl\"\n",
    "\n",
    "        if not model_path.exists() or not scaler_path.exists():\n",
    "            print(f\"Model files not found for {model_name}\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            self.models[model_name] = joblib.load(model_path)\n",
    "            self.scalers[model_name] = joblib.load(scaler_path)\n",
    "            print(f\"Loaded {model_name} model from {model_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_name} model: {e}\")\n",
    "            return False\n",
    "\n",
    "print(\"✓ ADMETSafetyModel base class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Methods - Toxicity Model\n",
    "\n",
    "Train a model to predict general toxicity using the **Tox21** dataset with 12 toxicity targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_toxicity_model(self, test_size: float = 0.2, random_state: int = 42) -> Dict:\n",
    "    \"\"\"\n",
    "    Train toxicity prediction model using Tox21 dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING TOXICITY MODEL (Tox21)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Load Tox21 dataset\n",
    "    df = self.load_dataset('tox21')\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    # Tox21 has 12 toxicity targets\n",
    "    target_cols = [col for col in df.columns if col.startswith('NR-') or col.startswith('SR-')]\n",
    "    smiles_col = 'smiles'\n",
    "\n",
    "    # Prepare data\n",
    "    X, y, valid_df = self.prepare_data(df, smiles_col, target_cols)\n",
    "\n",
    "    # Create binary toxicity label: toxic if any target is 1\n",
    "    y_binary = (y.sum(axis=1) > 0).astype(int)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_binary, test_size=test_size, random_state=random_state, stratify=y_binary\n",
    "    )\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train Random Forest model\n",
    "    print(\"\\nTraining Random Forest Classifier...\")\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    results = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'n_train': len(X_train),\n",
    "        'n_test': len(X_test)\n",
    "    }\n",
    "\n",
    "    # Save model\n",
    "    self.models['toxicity'] = model\n",
    "    self.scalers['toxicity'] = scaler\n",
    "    self.save_model('toxicity', model, scaler)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TOXICITY MODEL RESULTS\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Accuracy:  {results['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {results['precision']:.4f}\")\n",
    "    print(f\"Recall:    {results['recall']:.4f}\")\n",
    "    print(f\"F1-Score:  {results['f1']:.4f}\")\n",
    "    print(f\"ROC-AUC:   {results['roc_auc']:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(results['confusion_matrix'])\n",
    "    print(f\"\\nTrain samples: {results['n_train']}\")\n",
    "    print(f\"Test samples:  {results['n_test']}\")\n",
    "\n",
    "    self.results['toxicity'] = results\n",
    "    return results\n",
    "\n",
    "# Add method to class\n",
    "ADMETSafetyModel.train_toxicity_model = train_toxicity_model\n",
    "print(\"✓ Toxicity training method added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Methods - Clinical Toxicity Model\n",
    "\n",
    "Train a model to predict clinical trial toxicity using the **ClinTox** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clintox_model(self, test_size: float = 0.2, random_state: int = 42) -> Dict:\n",
    "    \"\"\"\n",
    "    Train clinical toxicity prediction model using ClinTox dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING CLINICAL TOXICITY MODEL (ClinTox)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    df = self.load_dataset('clintox')\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    target_cols = ['CT_TOX']\n",
    "    smiles_col = 'smiles'\n",
    "\n",
    "    X, y, valid_df = self.prepare_data(df, smiles_col, target_cols)\n",
    "    y = y.ravel()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(\"\\nTraining Random Forest Classifier...\")\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=20, min_samples_split=5,\n",
    "        min_samples_leaf=2, random_state=random_state, n_jobs=-1, verbose=0\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    results = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'n_train': len(X_train),\n",
    "        'n_test': len(X_test)\n",
    "    }\n",
    "\n",
    "    self.models['clintox'] = model\n",
    "    self.scalers['clintox'] = scaler\n",
    "    self.save_model('clintox', model, scaler)\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"CLINICAL TOXICITY MODEL RESULTS\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Accuracy:  {results['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {results['precision']:.4f}\")\n",
    "    print(f\"Recall:    {results['recall']:.4f}\")\n",
    "    print(f\"F1-Score:  {results['f1']:.4f}\")\n",
    "    print(f\"ROC-AUC:   {results['roc_auc']:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(results['confusion_matrix'])\n",
    "\n",
    "    self.results['clintox'] = results\n",
    "    return results\n",
    "\n",
    "ADMETSafetyModel.train_clintox_model = train_clintox_model\n",
    "print(\"✓ ClinTox training method added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Methods - BBB Permeability Model\n",
    "\n",
    "Train a model to predict Blood-Brain Barrier permeability using the **BBBP** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bbbp_model(self, test_size: float = 0.2, random_state: int = 42) -> Dict:\n",
    "    \"\"\"\n",
    "    Train Blood-Brain Barrier Permeability model using BBBP dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING BBB PERMEABILITY MODEL (BBBP)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    df = self.load_dataset('bbbp')\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    target_cols = ['p_np']\n",
    "    smiles_col = 'smiles'\n",
    "\n",
    "    X, y, valid_df = self.prepare_data(df, smiles_col, target_cols)\n",
    "    y = y.ravel()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(\"\\nTraining Random Forest Classifier...\")\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=20, min_samples_split=5,\n",
    "        min_samples_leaf=2, random_state=random_state, n_jobs=-1, verbose=0\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    results = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'n_train': len(X_train),\n",
    "        'n_test': len(X_test)\n",
    "    }\n",
    "\n",
    "    self.models['bbbp'] = model\n",
    "    self.scalers['bbbp'] = scaler\n",
    "    self.save_model('bbbp', model, scaler)\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"BBB PERMEABILITY MODEL RESULTS\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Accuracy:  {results['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {results['precision']:.4f}\")\n",
    "    print(f\"Recall:    {results['recall']:.4f}\")\n",
    "    print(f\"F1-Score:  {results['f1']:.4f}\")\n",
    "    print(f\"ROC-AUC:   {results['roc_auc']:.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(results['confusion_matrix'])\n",
    "\n",
    "    self.results['bbbp'] = results\n",
    "    return results\n",
    "\n",
    "ADMETSafetyModel.train_bbbp_model = train_bbbp_model\n",
    "print(\"✓ BBBP training method added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Methods - Solubility Model\n",
    "\n",
    "Train a regression model to predict aqueous solubility using the **ESOL (Delaney)** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_solubility_model(self, test_size: float = 0.2, random_state: int = 42) -> Dict:\n",
    "    \"\"\"\n",
    "    Train aqueous solubility prediction model using ESOL (Delaney) dataset.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING SOLUBILITY MODEL (ESOL/Delaney)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    df = self.load_dataset('delaney')\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    target_cols = ['measured log solubility in mols per litre']\n",
    "    smiles_col = 'smiles'\n",
    "\n",
    "    X, y, valid_df = self.prepare_data(df, smiles_col, target_cols)\n",
    "    y = y.ravel()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(\"\\nTraining Random Forest Regressor...\")\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=100, max_depth=20, min_samples_split=5,\n",
    "        min_samples_leaf=2, random_state=random_state, n_jobs=-1, verbose=0\n",
    "    )\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    results = {\n",
    "        'r2': r2_score(y_test, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'mae': mean_absolute_error(y_test, y_pred),\n",
    "        'n_train': len(X_train),\n",
    "        'n_test': len(X_test)\n",
    "    }\n",
    "\n",
    "    self.models['solubility'] = model\n",
    "    self.scalers['solubility'] = scaler\n",
    "    self.save_model('solubility', model, scaler)\n",
    "\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"SOLUBILITY MODEL RESULTS\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"R² Score:  {results['r2']:.4f}\")\n",
    "    print(f\"RMSE:      {results['rmse']:.4f}\")\n",
    "    print(f\"MAE:       {results['mae']:.4f}\")\n",
    "    print(f\"\\nTrain samples: {results['n_train']}\")\n",
    "    print(f\"Test samples:  {results['n_test']}\")\n",
    "\n",
    "    self.results['solubility'] = results\n",
    "    return results\n",
    "\n",
    "ADMETSafetyModel.train_solubility_model = train_solubility_model\n",
    "print(\"✓ Solubility training method added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train All Models Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_models(self) -> Dict:\n",
    "    \"\"\"\n",
    "    Train all ADMET models.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING ALL ADMET MODELS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    all_results = {}\n",
    "\n",
    "    models_to_train = [\n",
    "        ('toxicity', self.train_toxicity_model),\n",
    "        ('clintox', self.train_clintox_model),\n",
    "        ('bbbp', self.train_bbbp_model),\n",
    "        ('solubility', self.train_solubility_model)\n",
    "    ]\n",
    "\n",
    "    for model_name, train_func in models_to_train:\n",
    "        try:\n",
    "            result = train_func()\n",
    "            if result:\n",
    "                all_results[model_name] = result\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError training {model_name} model: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    for model_name, result in all_results.items():\n",
    "        print(f\"\\n{model_name.upper()}:\")\n",
    "        if 'accuracy' in result:\n",
    "            print(f\"  Accuracy: {result['accuracy']:.4f}\")\n",
    "            print(f\"  ROC-AUC:  {result['roc_auc']:.4f}\")\n",
    "        elif 'r2' in result:\n",
    "            print(f\"  R² Score: {result['r2']:.4f}\")\n",
    "            print(f\"  RMSE:     {result['rmse']:.4f}\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "ADMETSafetyModel.train_all_models = train_all_models\n",
    "print(\"✓ Train all models method added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prediction Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_admet(self, smiles: Union[str, List[str]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Predict ADMET properties for given SMILES.\n",
    "    \"\"\"\n",
    "    if isinstance(smiles, str):\n",
    "        smiles = [smiles]\n",
    "\n",
    "    results = {\n",
    "        'smiles': smiles,\n",
    "        'predictions': []\n",
    "    }\n",
    "\n",
    "    for smile in smiles:\n",
    "        descriptors = MolecularDescriptorCalculator.calculate_descriptors(smile)\n",
    "\n",
    "        if descriptors is None:\n",
    "            results['predictions'].append({\n",
    "                'valid': False,\n",
    "                'error': 'Invalid SMILES or descriptor calculation failed'\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        descriptors = descriptors.reshape(1, -1)\n",
    "        prediction = {'valid': True}\n",
    "\n",
    "        for model_name in ['toxicity', 'clintox', 'bbbp', 'solubility']:\n",
    "            if model_name in self.models:\n",
    "                model = self.models[model_name]\n",
    "                scaler = self.scalers[model_name]\n",
    "                X_scaled = scaler.transform(descriptors)\n",
    "\n",
    "                if model_name == 'solubility':\n",
    "                    pred = model.predict(X_scaled)[0]\n",
    "                    prediction[model_name] = float(pred)\n",
    "                else:\n",
    "                    pred_class = model.predict(X_scaled)[0]\n",
    "                    pred_proba = model.predict_proba(X_scaled)[0]\n",
    "                    prediction[model_name] = {\n",
    "                        'class': int(pred_class),\n",
    "                        'probability': float(pred_proba[1])\n",
    "                    }\n",
    "\n",
    "        results['predictions'].append(prediction)\n",
    "\n",
    "    return results\n",
    "\n",
    "ADMETSafetyModel.predict_admet = predict_admet\n",
    "print(\"✓ Prediction method added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Initialize and Train Models\n",
    "\n",
    "Now let's initialize the ADMET model and train all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(\"=\"*80)\n",
    "print(\"ADMET SAFETY MODEL - DRUG DISCOVERY PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "admet_model = ADMETSafetyModel()\n",
    "\n",
    "# Train all models\n",
    "print(\"\\nStarting model training...\")\n",
    "results = admet_model.train_all_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Predictions\n",
    "\n",
    "Test the trained models with known drug compounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test with common drug compounds\n",
    "test_smiles = [\n",
    "    \"CC(C)Cc1ccc(cc1)C(C)C(O)=O\",  # Ibuprofen\n",
    "    \"CC(=O)Oc1ccccc1C(=O)O\",  # Aspirin\n",
    "    \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\"  # Caffeine\n",
    "]\n",
    "\n",
    "drug_names = [\"Ibuprofen\", \"Aspirin\", \"Caffeine\"]\n",
    "\n",
    "print(\"\\nTest compounds:\")\n",
    "for i, (name, smile) in enumerate(zip(drug_names, test_smiles), 1):\n",
    "    print(f\"{i}. {name}: {smile}\")\n",
    "\n",
    "predictions = admet_model.predict_admet(test_smiles)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (name, smile, pred) in enumerate(zip(drug_names, predictions['smiles'], predictions['predictions']), 1):\n",
    "    print(f\"\\n{i}. {name}\")\n",
    "    print(f\"   SMILES: {smile}\")\n",
    "    if pred['valid']:\n",
    "        print(f\"   \\n   ADMET Properties:\")\n",
    "        for prop, value in pred.items():\n",
    "            if prop != 'valid':\n",
    "                if isinstance(value, dict):\n",
    "                    print(f\"     - {prop}: Class={value['class']}, Probability={value['probability']:.4f}\")\n",
    "                else:\n",
    "                    print(f\"     - {prop}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"   Error: {pred['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "Display final summary of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADMET MODEL TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nModels saved in: {admet_model.model_dir.absolute()}\")\n",
    "print(\"\\nAvailable models:\")\n",
    "for model_name in admet_model.models.keys():\n",
    "    print(f\"  ✓ {model_name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. Use these models to filter compounds in your drug discovery pipeline\")\n",
    "print(\"2. Integrate with Stage 1 (Target Prediction) and Stage 3 (Activity Prediction)\")\n",
    "print(\"3. Deploy models for production use via Streamlit or API\")\n",
    "print(\"4. Continue refining models with additional data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
